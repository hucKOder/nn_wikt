{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv('./data/'+ 'x_train' +'.csv', header=None)\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, batch_size):\n",
    "        self.x = Data.normalize(Data.import_data('x_train'))\n",
    "        self.y = Data.convert(Data.import_data('y_train'))\n",
    "        self.val_x = Data.normalize(Data.import_data('x_validation'))\n",
    "        self.val_y = Data.convert(Data.import_data('y_validation'))\n",
    "        self.pointer = 0\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def normalize(data):\n",
    "        mean, std = data.mean(), data.std()\n",
    "        data = (data - mean) / std\n",
    "        return data\n",
    "    \n",
    "    def scale(data):\n",
    "        return preprocessing.scale(data)\n",
    "    \n",
    "    def shuffle_data(self):\n",
    "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        batch = (self.x[self.pointer:self.pointer + self.batch_size], self.y[self.pointer:self.pointer + self.batch_size])\n",
    "        self.pointer += self.batch_size\n",
    "        return batch[0], batch[1]\n",
    "    \n",
    "    def convert(data):\n",
    "        return pd.get_dummies(data, columns = [0]).values\n",
    "        \n",
    "    def import_data(name):\n",
    "        return pd.read_csv('./data/'+ name +'.csv', header=None)\n",
    "    \n",
    "    def valid_data(self):\n",
    "        return self.val_x\n",
    "    \n",
    "    def label_data(self):\n",
    "        return self.val_y\n",
    "    \n",
    "    def move_pointer(self):\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def get_pointer(self):\n",
    "        return self.pointer\n",
    "    \n",
    "    def get_train_size(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/x_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6d1a87e2bb96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlogs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./logs\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Network Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f3a96aa5c698>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_validation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f3a96aa5c698>\u001b[0m in \u001b[0;36mimport_data\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thedarkside\\mnist_set\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thedarkside\\mnist_set\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thedarkside\\mnist_set\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thedarkside\\mnist_set\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thedarkside\\mnist_set\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1684\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./data/x_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "learning_rate = 0.001\n",
    "reg_constant = 0.001\n",
    "#Parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "logs_path = \"./logs\"\n",
    "\n",
    "data = Data(batch_size)\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 32 # data input (img shape: 8*4)\n",
    "num_classes = 2 # total classes (0-1)\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # Graph inputs\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    #dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "with tf.name_scope(\"weights\"):\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, num_classes]))\n",
    "    }\n",
    "with tf.name_scope(\"biases\"):\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([num_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    with tf.name_scope('hidden_layer_1'):\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        #dropout = tf.nn.dropout(layer_1, dropout_prob)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    with tf.name_scope('hidden_layer_2'):\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        #dropout_2 = tf.nn.dropout(layer_2, dropout_prob)\n",
    "        \n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    with tf.name_scope('ouput_layer_1'):\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss_op = tf.reduce_mean(entropy + reg_constant*tf.nn.l2_loss(weights['h1']) +\n",
    "                         reg_constant*tf.nn.l2_loss(weights['h2']) + reg_constant*tf.nn.l2_loss(weights['out']))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Testing Accuracy:  0.83325 Loss:  35.4056\n",
      "Epoch:  2 Testing Accuracy:  0.81595 Loss:  29.7051\n",
      "Epoch:  3 Testing Accuracy:  0.8777 Loss:  25.1971\n",
      "Epoch:  4 Testing Accuracy:  0.89175 Loss:  21.762\n",
      "Epoch:  5 Testing Accuracy:  0.8614 Loss:  19.1768\n",
      "Epoch:  6 Testing Accuracy:  0.8303 Loss:  17.3535\n",
      "Epoch:  7 Testing Accuracy:  0.8082 Loss:  15.0265\n",
      "Epoch:  8 Testing Accuracy:  0.88575 Loss:  12.1134\n",
      "Epoch:  9 Testing Accuracy:  0.83735 Loss:  10.5693\n",
      "Epoch:  10 Testing Accuracy:  0.9066 Loss:  9.16552\n",
      "Epoch:  11 Testing Accuracy:  0.86975 Loss:  7.17789\n",
      "Epoch:  12 Testing Accuracy:  0.90335 Loss:  5.50037\n",
      "Epoch:  13 Testing Accuracy:  0.90125 Loss:  4.30814\n",
      "Epoch:  14 Testing Accuracy:  0.8968 Loss:  3.28877\n",
      "Epoch:  15 Testing Accuracy:  0.91235 Loss:  2.38052\n",
      "Epoch:  16 Testing Accuracy:  0.92985 Loss:  1.65431\n",
      "Epoch:  17 Testing Accuracy:  0.92465 Loss:  1.1878\n",
      "Epoch:  18 Testing Accuracy:  0.93045 Loss:  0.820852\n",
      "Epoch:  19 Testing Accuracy:  0.93165 Loss:  0.609227\n",
      "Epoch:  20 Testing Accuracy:  0.84495 Loss:  0.615552\n",
      "Epoch:  21 Testing Accuracy:  0.9404 Loss:  0.384739\n",
      "Epoch:  22 Testing Accuracy:  0.94215 Loss:  0.337528\n",
      "Epoch:  23 Testing Accuracy:  0.94805 Loss:  0.295921\n",
      "Epoch:  24 Testing Accuracy:  0.9229 Loss:  0.32502\n",
      "Epoch:  25 Testing Accuracy:  0.94475 Loss:  0.274376\n",
      "Epoch:  26 Testing Accuracy:  0.93995 Loss:  0.262893\n",
      "Epoch:  27 Testing Accuracy:  0.94705 Loss:  0.245539\n",
      "Epoch:  28 Testing Accuracy:  0.9488 Loss:  0.236351\n",
      "Epoch:  29 Testing Accuracy:  0.94625 Loss:  0.235187\n",
      "Epoch:  30 Testing Accuracy:  0.94875 Loss:  0.244422\n",
      "Epoch:  31 Testing Accuracy:  0.95015 Loss:  0.234181\n",
      "Epoch:  32 Testing Accuracy:  0.94845 Loss:  0.227687\n",
      "Epoch:  33 Testing Accuracy:  0.95215 Loss:  0.230813\n",
      "Epoch:  34 Testing Accuracy:  0.94995 Loss:  0.228023\n",
      "Epoch:  35 Testing Accuracy:  0.94805 Loss:  0.231944\n",
      "Epoch:  36 Testing Accuracy:  0.94955 Loss:  0.223337\n",
      "Epoch:  37 Testing Accuracy:  0.94885 Loss:  0.233942\n",
      "Epoch:  38 Testing Accuracy:  0.94605 Loss:  0.234701\n",
      "Epoch:  39 Testing Accuracy:  0.9493 Loss:  0.223608\n",
      "Epoch:  40 Testing Accuracy:  0.9452 Loss:  0.23122\n",
      "Epoch:  41 Testing Accuracy:  0.9523 Loss:  0.228944\n",
      "Epoch:  42 Testing Accuracy:  0.95055 Loss:  0.226699\n",
      "Epoch:  43 Testing Accuracy:  0.95045 Loss:  0.221915\n",
      "Epoch:  44 Testing Accuracy:  0.95355 Loss:  0.220942\n",
      "Epoch:  45 Testing Accuracy:  0.94965 Loss:  0.222782\n",
      "Epoch:  46 Testing Accuracy:  0.9503 Loss:  0.227448\n",
      "Epoch:  47 Testing Accuracy:  0.9456 Loss:  0.235018\n",
      "Epoch:  48 Testing Accuracy:  0.9516 Loss:  0.219548\n",
      "Epoch:  49 Testing Accuracy:  0.95345 Loss:  0.221818\n",
      "Epoch:  50 Testing Accuracy:  0.95205 Loss:  0.222072\n",
      "Epoch:  51 Testing Accuracy:  0.9499 Loss:  0.224608\n",
      "Epoch:  52 Testing Accuracy:  0.9542 Loss:  0.220358\n",
      "Epoch:  53 Testing Accuracy:  0.94815 Loss:  0.227054\n",
      "Epoch:  54 Testing Accuracy:  0.9561 Loss:  0.215546\n",
      "Epoch:  55 Testing Accuracy:  0.95135 Loss:  0.219293\n",
      "Epoch:  56 Testing Accuracy:  0.95155 Loss:  0.220283\n",
      "Epoch:  57 Testing Accuracy:  0.9499 Loss:  0.220152\n",
      "Epoch:  58 Testing Accuracy:  0.9495 Loss:  0.230682\n",
      "Epoch:  59 Testing Accuracy:  0.9527 Loss:  0.218898\n",
      "Epoch:  60 Testing Accuracy:  0.95665 Loss:  0.215723\n",
      "Epoch:  61 Testing Accuracy:  0.952 Loss:  0.218801\n",
      "Epoch:  62 Testing Accuracy:  0.9482 Loss:  0.220997\n",
      "Epoch:  63 Testing Accuracy:  0.95515 Loss:  0.215262\n",
      "Epoch:  64 Testing Accuracy:  0.95445 Loss:  0.214741\n",
      "Epoch:  65 Testing Accuracy:  0.9496 Loss:  0.221389\n",
      "Epoch:  66 Testing Accuracy:  0.9562 Loss:  0.212151\n",
      "Epoch:  67 Testing Accuracy:  0.95395 Loss:  0.212385\n",
      "Epoch:  68 Testing Accuracy:  0.9492 Loss:  0.217646\n",
      "Epoch:  69 Testing Accuracy:  0.9515 Loss:  0.220304\n",
      "Epoch:  70 Testing Accuracy:  0.9546 Loss:  0.213108\n",
      "Epoch:  71 Testing Accuracy:  0.95275 Loss:  0.222133\n",
      "Epoch:  72 Testing Accuracy:  0.9541 Loss:  0.22148\n",
      "Epoch:  73 Testing Accuracy:  0.94825 Loss:  0.224166\n",
      "Epoch:  74 Testing Accuracy:  0.95485 Loss:  0.212196\n",
      "Epoch:  75 Testing Accuracy:  0.95155 Loss:  0.221799\n",
      "Epoch:  76 Testing Accuracy:  0.94625 Loss:  0.232732\n",
      "Epoch:  77 Testing Accuracy:  0.95705 Loss:  0.217382\n",
      "Epoch:  78 Testing Accuracy:  0.9507 Loss:  0.223097\n",
      "Epoch:  79 Testing Accuracy:  0.95535 Loss:  0.216335\n",
      "Epoch:  80 Testing Accuracy:  0.95055 Loss:  0.220905\n",
      "Epoch:  81 Testing Accuracy:  0.95455 Loss:  0.219452\n",
      "Epoch:  82 Testing Accuracy:  0.9491 Loss:  0.22347\n",
      "Epoch:  83 Testing Accuracy:  0.94945 Loss:  0.223341\n",
      "Epoch:  84 Testing Accuracy:  0.95155 Loss:  0.220113\n",
      "Epoch:  85 Testing Accuracy:  0.95395 Loss:  0.217257\n",
      "Epoch:  86 Testing Accuracy:  0.9513 Loss:  0.216166\n",
      "Epoch:  87 Testing Accuracy:  0.95065 Loss:  0.218714\n",
      "Epoch:  88 Testing Accuracy:  0.9508 Loss:  0.221622\n",
      "Epoch:  89 Testing Accuracy:  0.9521 Loss:  0.21477\n",
      "Epoch:  90 Testing Accuracy:  0.9547 Loss:  0.217052\n",
      "Epoch:  91 Testing Accuracy:  0.9493 Loss:  0.219662\n",
      "Epoch:  92 Testing Accuracy:  0.9507 Loss:  0.218889\n",
      "Epoch:  93 Testing Accuracy:  0.95035 Loss:  0.221948\n",
      "Epoch:  94 Testing Accuracy:  0.95275 Loss:  0.213196\n",
      "Epoch:  95 Testing Accuracy:  0.94865 Loss:  0.232584\n",
      "Epoch:  96 Testing Accuracy:  0.9554 Loss:  0.216717\n",
      "Epoch:  97 Testing Accuracy:  0.95255 Loss:  0.219813\n",
      "Epoch:  98 Testing Accuracy:  0.95255 Loss:  0.2129\n",
      "Epoch:  99 Testing Accuracy:  0.95235 Loss:  0.2151\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    test_writer = tf.summary.FileWriter(logs_path + '/train/' + 'final_2', sess.graph)\n",
    "    for step in range(1, epochs):\n",
    "        while data.get_train_size() > batch_size + data.get_pointer():\n",
    "            batch_x, batch_y = data.next_batch()\n",
    "            train.run(feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "        valid_x = data.valid_data()\n",
    "        valid_y = data.label_data()\n",
    "        summary, test_acc, loss_val = sess.run([merged, accuracy, loss_op], feed_dict={X: valid_x, Y: valid_y})\n",
    "        test_writer.add_summary(summary, step)\n",
    "        print(\"Epoch: \", step, \"Testing Accuracy: \", test_acc, \"Loss: \", loss_val)\n",
    "        \n",
    "        data.shuffle_data()\n",
    "        data.move_pointer()\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    test_data = pd.read_csv('./data/x_test.csv', header=None)\n",
    "    results = sess.run(logits, feed_dict={X: test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
