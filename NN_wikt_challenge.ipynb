{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforwad NN for wikt challenge\n",
    "\n",
    "by Andrej Hucko\n",
    "\n",
    "Tento model sme nestihli odovzdat do sutaze. Dava velmi dobre vysledky.\n",
    "Venoval som tomu najviac casu s pomedzi ostatnych hodnotenych casti 2_projektu kedze na BP sa budem venovat neuronovym sietiam \n",
    "a tento model mal sluzit ako moj uvod do Tensorflow, implementacie a optimalizacie neuronovych sieti pre urcite problemy.\n",
    "\n",
    "Pouzili sme jednoduchu doprednu siet s dvomi vrstvami. 2048 skrytych neuronov v kazdej skrytej vrstve. Ako vystup s output layer\n",
    "sme pouzili tradicnu softmax funkciu.\n",
    "\n",
    "#### Parametre:\n",
    "learning_rate = 0.001\n",
    "reg_constant = 0.001\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "Namiesto GradientDescent sme pouzili AdamOptimizer, ktory dokaze pocas trenovania dynamicky upravovat hodnotu learning rate.\n",
    "Ako aktivacnu funkciu sme zvolili ReLU.\n",
    "Ako nasu chybovu funkciu sme zvolili krizovu entropiu.\n",
    "Data boli normalizovane pre spracovanim.\n",
    "Ako vyhodnocovaciu metriku sme zvolili accuracy. A vyhodnocovali sme ju na validacnej vzorke.\n",
    "\n",
    "#### Tensorboard\n",
    "\n",
    "pomocou prikazu v cmd: \"tensorboard --logdir=./logs/prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, batch_size):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.x = Data.import_data('x_train')\n",
    "        self.y = Data.convert(Data.import_data('y_train'))\n",
    "        self.val_x = Data.import_data('x_validation')\n",
    "        self.val_y = Data.convert(Data.import_data('y_validation'))\n",
    "        self.test_x = Data.import_data('x_test')\n",
    "        self.pointer = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.scaler.fit(self.x)\n",
    "        self.scaler.transform(self.x)\n",
    "        self.scaler.transform(self.val_x)\n",
    "        self.scaler.transform(self.test_x)\n",
    "    \n",
    "    def shuffle_data(self):\n",
    "        self.x, self.y = shuffle(self.x, self.y, random_state=0)\n",
    "    \n",
    "    def next_batch(self):\n",
    "        batch = (self.x[self.pointer:self.pointer + self.batch_size], self.y[self.pointer:self.pointer + self.batch_size])\n",
    "        self.pointer += self.batch_size\n",
    "        return batch[0], batch[1]\n",
    "    \n",
    "    def convert(data):\n",
    "        return pd.get_dummies(data, columns = [0]).values\n",
    "        \n",
    "    def import_data(name):\n",
    "        return pd.read_csv('../data/'+ name +'.csv', header=None)\n",
    "    \n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getTest(self):\n",
    "        return self.test_x\n",
    "    \n",
    "    def valid_data(self):\n",
    "        return self.val_x\n",
    "    \n",
    "    def label_data(self):\n",
    "        return self.val_y\n",
    "    \n",
    "    def move_pointer(self):\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def get_pointer(self):\n",
    "        return self.pointer\n",
    "    \n",
    "    def get_train_size(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.y\n",
    "    \n",
    "    def get_scaler(self):\n",
    "        return self.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "learning_rate = 0.001\n",
    "reg_constant = 0.001\n",
    "#Parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "logs_path = \"./logs\"\n",
    "\n",
    "data = Data(batch_size)\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 2048 # 1st layer number of neurons\n",
    "n_hidden_2 = 2048 # 2nd layer number of neurons\n",
    "num_input = 32 # data input (img shape: 8*4)\n",
    "num_classes = 2 # total classes (0-1)\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    # Graph inputs\n",
    "    X = tf.placeholder(\"float\", [None, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    #dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "with tf.name_scope(\"weights\"):\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, num_classes]))\n",
    "    }\n",
    "with tf.name_scope(\"biases\"):\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.truncated_normal([num_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 2048 neurons\n",
    "    with tf.name_scope('hidden_layer_1'):\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        #dropout = tf.nn.dropout(layer_1, dropout_prob)\n",
    "    # Hidden fully connected layer with 2048 neurons\n",
    "    with tf.name_scope('hidden_layer_2'):\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        #dropout_2 = tf.nn.dropout(layer_2, dropout_prob)\n",
    "        \n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    with tf.name_scope('ouput_layer_1'):\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss_op = tf.reduce_mean(entropy + reg_constant*tf.nn.l2_loss(weights['h1']) +\n",
    "                         reg_constant*tf.nn.l2_loss(weights['h2']) + reg_constant*tf.nn.l2_loss(weights['out']))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Testing Accuracy:  0.8705 Loss:  1755.89\n",
      "Epoch:  2 Testing Accuracy:  0.88445 Loss:  1665.59\n",
      "Epoch:  3 Testing Accuracy:  0.89445 Loss:  1414.42\n",
      "Epoch:  4 Testing Accuracy:  0.85385 Loss:  1356.27\n",
      "Epoch:  5 Testing Accuracy:  0.9029 Loss:  1170.81\n",
      "Epoch:  6 Testing Accuracy:  0.88845 Loss:  1154.06\n",
      "Epoch:  7 Testing Accuracy:  0.9002 Loss:  1011.05\n",
      "Epoch:  8 Testing Accuracy:  0.9218 Loss:  988.883\n",
      "Epoch:  9 Testing Accuracy:  0.8181 Loss:  990.998\n",
      "Epoch:  10 Testing Accuracy:  0.92935 Loss:  827.862\n",
      "Epoch:  11 Testing Accuracy:  0.9255 Loss:  780.566\n",
      "Epoch:  12 Testing Accuracy:  0.93585 Loss:  716.612\n",
      "Epoch:  13 Testing Accuracy:  0.899 Loss:  654.883\n",
      "Epoch:  14 Testing Accuracy:  0.902 Loss:  636.048\n",
      "Epoch:  15 Testing Accuracy:  0.93095 Loss:  557.203\n",
      "Epoch:  16 Testing Accuracy:  0.91575 Loss:  541.112\n",
      "Epoch:  17 Testing Accuracy:  0.9387 Loss:  515.919\n",
      "Epoch:  18 Testing Accuracy:  0.92765 Loss:  465.04\n",
      "Epoch:  19 Testing Accuracy:  0.92405 Loss:  438.212\n",
      "Epoch:  20 Testing Accuracy:  0.8339 Loss:  474.978\n",
      "Epoch:  21 Testing Accuracy:  0.88935 Loss:  393.237\n",
      "Epoch:  22 Testing Accuracy:  0.92235 Loss:  354.979\n",
      "Epoch:  23 Testing Accuracy:  0.9461 Loss:  328.801\n",
      "Epoch:  24 Testing Accuracy:  0.9271 Loss:  309.103\n",
      "Epoch:  25 Testing Accuracy:  0.9188 Loss:  283.932\n",
      "Epoch:  26 Testing Accuracy:  0.94325 Loss:  257.155\n",
      "Epoch:  27 Testing Accuracy:  0.94395 Loss:  241.76\n",
      "Epoch:  28 Testing Accuracy:  0.93315 Loss:  232.407\n",
      "Epoch:  29 Testing Accuracy:  0.95385 Loss:  210.504\n",
      "Epoch:  30 Testing Accuracy:  0.9372 Loss:  193.79\n",
      "Epoch:  31 Testing Accuracy:  0.92965 Loss:  187.363\n",
      "Epoch:  32 Testing Accuracy:  0.9457 Loss:  171.757\n",
      "Epoch:  33 Testing Accuracy:  0.94545 Loss:  160.13\n",
      "Epoch:  34 Testing Accuracy:  0.9514 Loss:  142.26\n",
      "Epoch:  35 Testing Accuracy:  0.93455 Loss:  134.597\n",
      "Epoch:  36 Testing Accuracy:  0.94965 Loss:  131.443\n",
      "Epoch:  37 Testing Accuracy:  0.88645 Loss:  123.358\n",
      "Epoch:  38 Testing Accuracy:  0.9438 Loss:  104.504\n",
      "Epoch:  39 Testing Accuracy:  0.92905 Loss:  97.7766\n",
      "Epoch:  40 Testing Accuracy:  0.95835 Loss:  88.6923\n",
      "Epoch:  41 Testing Accuracy:  0.95165 Loss:  84.2618\n",
      "Epoch:  42 Testing Accuracy:  0.95205 Loss:  75.035\n",
      "Epoch:  43 Testing Accuracy:  0.95185 Loss:  69.9257\n",
      "Epoch:  44 Testing Accuracy:  0.9389 Loss:  63.8183\n",
      "Epoch:  45 Testing Accuracy:  0.9475 Loss:  62.6105\n",
      "Epoch:  46 Testing Accuracy:  0.9201 Loss:  54.8639\n",
      "Epoch:  47 Testing Accuracy:  0.94815 Loss:  48.1418\n",
      "Epoch:  48 Testing Accuracy:  0.9586 Loss:  44.5401\n",
      "Epoch:  49 Testing Accuracy:  0.9585 Loss:  40.461\n",
      "Epoch:  50 Testing Accuracy:  0.95365 Loss:  37.105\n",
      "Epoch:  51 Testing Accuracy:  0.95265 Loss:  33.5179\n",
      "Epoch:  52 Testing Accuracy:  0.94005 Loss:  29.5598\n",
      "Epoch:  53 Testing Accuracy:  0.95325 Loss:  26.3643\n",
      "Epoch:  54 Testing Accuracy:  0.941 Loss:  23.6632\n",
      "Epoch:  55 Testing Accuracy:  0.95235 Loss:  21.0444\n",
      "Epoch:  56 Testing Accuracy:  0.9319 Loss:  18.9248\n",
      "Epoch:  57 Testing Accuracy:  0.94535 Loss:  16.5714\n",
      "Epoch:  58 Testing Accuracy:  0.9322 Loss:  14.9109\n",
      "Epoch:  59 Testing Accuracy:  0.9254 Loss:  12.9066\n",
      "Epoch:  60 Testing Accuracy:  0.9336 Loss:  11.1016\n",
      "Epoch:  61 Testing Accuracy:  0.94175 Loss:  9.60419\n",
      "Epoch:  62 Testing Accuracy:  0.9442 Loss:  8.21314\n",
      "Epoch:  63 Testing Accuracy:  0.9321 Loss:  6.82048\n",
      "Epoch:  64 Testing Accuracy:  0.9309 Loss:  5.57958\n",
      "Epoch:  65 Testing Accuracy:  0.91065 Loss:  4.66649\n",
      "Epoch:  66 Testing Accuracy:  0.9404 Loss:  3.63755\n",
      "Epoch:  67 Testing Accuracy:  0.9454 Loss:  2.93047\n",
      "Epoch:  68 Testing Accuracy:  0.93835 Loss:  2.25437\n",
      "Epoch:  69 Testing Accuracy:  0.9394 Loss:  1.78846\n",
      "Epoch:  70 Testing Accuracy:  0.9413 Loss:  1.39636\n",
      "Epoch:  71 Testing Accuracy:  0.93125 Loss:  1.09491\n",
      "Epoch:  72 Testing Accuracy:  0.9433 Loss:  0.854239\n",
      "Epoch:  73 Testing Accuracy:  0.94345 Loss:  0.698815\n",
      "Epoch:  74 Testing Accuracy:  0.9369 Loss:  0.586387\n",
      "Epoch:  75 Testing Accuracy:  0.94155 Loss:  0.506958\n",
      "Epoch:  76 Testing Accuracy:  0.949 Loss:  0.426469\n",
      "Epoch:  77 Testing Accuracy:  0.9483 Loss:  0.383859\n",
      "Epoch:  78 Testing Accuracy:  0.94695 Loss:  0.345706\n",
      "Epoch:  79 Testing Accuracy:  0.95065 Loss:  0.317764\n",
      "Epoch:  80 Testing Accuracy:  0.95195 Loss:  0.299848\n",
      "Epoch:  81 Testing Accuracy:  0.9466 Loss:  0.288909\n",
      "Epoch:  82 Testing Accuracy:  0.94875 Loss:  0.271481\n",
      "Epoch:  83 Testing Accuracy:  0.95075 Loss:  0.258419\n",
      "Epoch:  84 Testing Accuracy:  0.9523 Loss:  0.247321\n",
      "Epoch:  85 Testing Accuracy:  0.95365 Loss:  0.242179\n",
      "Epoch:  86 Testing Accuracy:  0.95005 Loss:  0.243218\n",
      "Epoch:  87 Testing Accuracy:  0.9527 Loss:  0.232046\n",
      "Epoch:  88 Testing Accuracy:  0.9554 Loss:  0.22147\n",
      "Epoch:  89 Testing Accuracy:  0.9564 Loss:  0.214377\n",
      "Epoch:  90 Testing Accuracy:  0.95215 Loss:  0.227505\n",
      "Epoch:  91 Testing Accuracy:  0.9533 Loss:  0.218147\n",
      "Epoch:  92 Testing Accuracy:  0.95915 Loss:  0.206935\n",
      "Epoch:  93 Testing Accuracy:  0.951 Loss:  0.223752\n",
      "Epoch:  94 Testing Accuracy:  0.959 Loss:  0.200671\n",
      "Epoch:  95 Testing Accuracy:  0.95625 Loss:  0.21429\n",
      "Epoch:  96 Testing Accuracy:  0.9595 Loss:  0.200883\n",
      "Epoch:  97 Testing Accuracy:  0.9596 Loss:  0.195965\n",
      "Epoch:  98 Testing Accuracy:  0.9537 Loss:  0.209167\n",
      "Epoch:  99 Testing Accuracy:  0.96 Loss:  0.195962\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    test_writer = tf.summary.FileWriter(logs_path + '/prediction/' + 'tryhard', sess.graph)\n",
    "    for step in range(1, epochs):\n",
    "        while data.get_train_size() > batch_size + data.get_pointer():\n",
    "            batch_x, batch_y = data.next_batch()\n",
    "            train.run(feed_dict={X: batch_x, Y: batch_y})\n",
    "            \n",
    "        valid_x = data.valid_data()\n",
    "        valid_y = data.label_data()\n",
    "        summary, test_acc, loss_val = sess.run([merged, accuracy, loss_op], feed_dict={X: valid_x, Y: valid_y})\n",
    "        test_writer.add_summary(summary, step)\n",
    "        print(\"Epoch: \", step, \"Testing Accuracy: \", test_acc, \"Loss: \", loss_val)\n",
    "        \n",
    "        data.shuffle_data()\n",
    "        data.move_pointer()\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    results = sess.run(logits, feed_dict={X: data.getTest()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for i in results:\n",
    "    if i[0] > i[1]:\n",
    "        out.append(0)\n",
    "    else:\n",
    "        out.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = numpy.asarray(out)\n",
    "numpy.savetxt(\"predictions.csv\", a, fmt=\"%d\", delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
